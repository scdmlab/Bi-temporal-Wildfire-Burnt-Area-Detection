# GitHub Project Structure Documentation

This is a streamlined bi-temporal attention U-Net wildfire burnt area detection project structure suitable for uploading to GitHub.

## ğŸ“ Project Structure

```
bitemporal-attention-unet/
â”œâ”€â”€ ğŸ“‚ configs/                 # Configuration files
â”‚   â”œâ”€â”€ config.yaml            # Main configuration file
â”‚   â””â”€â”€ config_example.yaml    # Configuration example file
â”œâ”€â”€ ğŸ“‚ datasets/               # Dataset processing
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ bitemporal_dataset.py  # Bi-temporal dataset class
â”œâ”€â”€ ğŸ“‚ examples/               # Examples and tutorials
â”‚   â””â”€â”€ quick_start.ipynb      # Quick start Jupyter tutorial
â”œâ”€â”€ ğŸ“‚ models/                 # Model definitions
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ attention_unet.py      # Attention U-Net models
â”œâ”€â”€ ğŸ“‚ scripts/                # Training and inference scripts
â”‚   â”œâ”€â”€ train.py              # Training script
â”‚   â”œâ”€â”€ inference.py          # Inference script
â”‚   â””â”€â”€ evaluate.py           # Evaluation script
â”œâ”€â”€ ğŸ“‚ utils/                  # Utility functions
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ data_utils.py         # Data processing utilities
â”‚   â”œâ”€â”€ losses.py             # Loss functions
â”‚   â”œâ”€â”€ metrics.py            # Evaluation metrics
â”‚   â””â”€â”€ visualize.py          # Visualization tools
â”œâ”€â”€ ğŸ“„ .gitignore             # Git ignore file
â”œâ”€â”€ ğŸ“„ LICENSE                # Open source license
â”œâ”€â”€ ğŸ“„ README.md              # Project documentation
â””â”€â”€ ğŸ“„ requirements.txt       # Python dependencies
```

## ğŸ¯ Core Files Description

### ğŸ“‹ Model Files (`models/`)
- **`attention_unet.py`**: Contains all model architectures
  - `AttentionUNet`: Single-temporal attention U-Net
  - `BiTemporalUNet`: Bi-temporal U-Net
  - `BiTemporalAttentionUNet`: Bi-temporal attention U-Net (recommended)

### ğŸ“Š Data Processing (`datasets/`)
- **`bitemporal_dataset.py`**: Bi-temporal dataset loader
  - Supports multiple image formats
  - Automatic data augmentation
  - Flexible file naming

### ğŸš€ Scripts (`scripts/`)
- **`train.py`**: Complete training pipeline
- **`inference.py`**: Batch inference script
- **`evaluate.py`**: Model evaluation script

### ğŸ› ï¸ Utilities (`utils/`)
- **`losses.py`**: Multiple loss functions (BCE, Dice, Focal, Combined)
- **`metrics.py`**: Evaluation metrics (IoU, F1, AUC, etc.)
- **`visualize.py`**: Result visualization tools
- **`data_utils.py`**: Data preprocessing and analysis tools

### âš™ï¸ Configuration (`configs/`)
- **`config.yaml`**: Main configuration file
- **`config_example.yaml`**: Detailed configuration examples

## ğŸš€ Quick Start

### 1. Install Dependencies
```bash
pip install -r requirements.txt
```

### 2. Prepare Data
Organize your data in the following structure:
```
data/
â”œâ”€â”€ pre_fire/     # Pre-fire images
â”œâ”€â”€ post_fire/    # Post-fire images
â””â”€â”€ burnt_masks/  # Burnt area masks (optional)
```

### 3. Modify Configuration
Edit `configs/config.yaml` and update data paths:
```yaml
data:
  pre_dir: "/path/to/your/pre_fire/images"
  post_dir: "/path/to/your/post_fire/images"
  mask_dir: "/path/to/your/burnt_masks"
```

### 4. Train Model
```bash
python scripts/train.py --config configs/config.yaml --output-dir outputs/
```

### 5. Run Inference
```bash
python scripts/inference.py --config configs/config.yaml --model outputs/best_model.pth --pre-dir data/test/pre_fire --post-dir data/test/post_fire --output-dir results/
```

## ğŸ“š Features

### âœ¨ Model Architecture
- ğŸ¯ Bi-temporal architecture designed specifically for wildfire burnt area detection
- ğŸ” Attention mechanism for enhanced feature selection
- ğŸ—ï¸ Multi-scale feature fusion
- âš¡ Efficient memory usage

### ğŸ“Š Data Processing
- ğŸ”„ Automatic data augmentation
- ğŸ“ Flexible image size handling
- ğŸ—‚ï¸ Multiple file format support
- âœ… Data integrity checking

### ğŸ¨ Visualization
- ğŸ“ˆ Training curve plotting
- ğŸ–¼ï¸ Prediction result overlays
- ğŸ“Š Confusion matrices and ROC curves
- ğŸ” Attention map visualization

### ğŸ›ï¸ Configuration Flexibility
- ğŸ“ YAML configuration files
- ğŸ”§ Multiple loss function combinations
- ğŸ“Š Rich evaluation metrics
- âš™ï¸ Adjustable training parameters

## ğŸ“¦ Differences from Original Project

### ğŸ—‚ï¸ Streamlined Content
- âŒ Removed dataset files
- âŒ Removed temporary files and cache
- âŒ Removed experimental result files
- âŒ Removed redundant code

### âœ… Core Features Retained
- âœ… Complete model implementation
- âœ… Training and inference pipelines
- âœ… Evaluation and visualization tools
- âœ… Detailed documentation and examples

### ğŸ†• New Features
- ğŸ““ Jupyter tutorial examples
- ğŸ”§ Unified configuration management
- ğŸ“Š Enhanced visualization capabilities
- ğŸ› ï¸ Data preprocessing tools

## ğŸ“„ Estimated File Size

Estimated size of the entire GitHub project (excluding data):
- Core code: ~50KB
- Documentation and examples: ~100KB
- Configuration files: ~10KB
- **Total: <200KB**

Perfect for GitHub upload, fast cloning, easy sharing and collaboration.

## ğŸ¤ Usage Recommendations

1. After **Fork or Clone**, first read `README.md`
2. Run `examples/quick_start.ipynb` to understand basic usage
3. Modify `configs/config.yaml` according to your data
4. Use `utils/data_utils.py` to analyze your dataset
5. Start training and experimenting!

This structure maintains the complete functionality of the original project while being suitable for open source sharing and academic exchange.
